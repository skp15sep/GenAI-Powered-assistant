# GenAI-Powered-assistant
GenAI â€“ Powered Financial Insights Assistant â€“ Agile Design &amp; Testing Project

```markdown
# ğŸ’° Gen-Fin-Assistant-Gemini

## ğŸ“˜ Project Overview
**Gen-Fin-Assistant-Gemini** is a Generative AIâ€“powered financial assistant built using **FastAPI** and **Google Gemini/OpenAI APIs**.  
It helps users analyze and query their financial transaction data in **natural language**, using **RAG (Retrieval-Augmented Generation)** to generate accurate, data-backed insights.

Users can:
- Upload financial transaction data (CSV or DB)
- Ask questions like â€œHow much did I spend on groceries last month?â€
- Get intelligent answers and summaries generated by an LLM (Gemini / OpenAI GPT)
- View categorized spending insights and trends

---

## ğŸ§© Directory Structure
```

Gen-fin-assistant-gemeni/
â”‚
â”œâ”€â”€ app.py                # Main FastAPI application entrypoint
â”œâ”€â”€ openai_client.py      # Handles communication with OpenAI/Gemini API
â”œâ”€â”€ processor.py          # Data processing logic (load, clean, analyze, RAG pipeline)
â”œâ”€â”€ prompts.py            # Centralized prompt templates for consistent LLM queries
â”œâ”€â”€ requirements.txt      # Project dependencies
â””â”€â”€ README.md             # Project documentation

````

---

## âš™ï¸ Script Explanation

### 1ï¸âƒ£ `app.py` â€” **Main FastAPI Application**
- Acts as the backend entrypoint.
- Exposes the `/query` API endpoint for user questions.
- Loads `.env` (API key and config).
- Uses `processor.py` to load and analyze transaction data.
- Uses `openai_client.py` (Gemini/OpenAI) to get intelligent answers.
- Includes `/health` endpoint for service monitoring.

**Example logic flow:**
1. User sends question â†’ `/query`
2. App loads transaction data â†’ `processor.load_transactions_csv()`
3. Calls Gemini/OpenAI â†’ `openai_client.chat()`
4. Returns JSON answer to user

**Key Features:**
- Asynchronous FastAPI API
- CORS middleware for frontend integration
- Modular design for easy scaling

---

### 2ï¸âƒ£ `openai_client.py` â€” **LLM Communication Layer**
- Wraps the Gemini or OpenAI API for chat and embeddings.
- Handles authentication, retries, and error management.
- Provides clean methods like:
  ```python
  chat(messages)  # Send multi-turn conversations
  generate_text(prompt)  # Simple text prompt interface
  embed(text)  # Get text embeddings for vector search
````

* Supports model configuration:

  ```python
  model="gpt-4o-mini"  # or "gemini-1.5-flash"
  ```

**Responsibilities:**

* Connects to the selected LLM
* Manages API key securely via environment variable
* Handles rat


